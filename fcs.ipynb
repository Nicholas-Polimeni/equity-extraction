{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tables for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import utils\n",
    "\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "# --- Constants ---\n",
    "LARGE_CORP_MIN_SALES = 50\n",
    "MED_CORP_MIN_SALES = 10\n",
    "# -----------------\n",
    "\n",
    "def format_thousands(value, tick_number):\n",
    "    if value >= 1000:\n",
    "        value = int(round(value / 1000, 0))\n",
    "        return f\"{value}K\"\n",
    "    else:\n",
    "        return int(value)\n",
    "    \n",
    "def format_hundreds(value, tick_number):\n",
    "    if value >= 1000:\n",
    "        value = float(round(value / 1000, 1))\n",
    "        return f\"{value}K\"\n",
    "    else:\n",
    "        return int(value)\n",
    "\n",
    "def format_percent(value, tick_number):\n",
    "    return f\"{int(value)}%\"\n",
    "\n",
    "def format_millions(value, tick_number):\n",
    "    return f\"${int(value/1000000)}M\"\n",
    "\n",
    "# classify grantee/grantor by corp size\n",
    "def classify_corp(x):\n",
    "    if x > LARGE_CORP_MIN_SALES:\n",
    "        return 3\n",
    "    elif x > MED_CORP_MIN_SALES:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "DATA_PATH = 'output/'\n",
    "\n",
    "digest_atl = pd.read_csv(\"fcs_digest_atl.csv\")\n",
    "sales_atl = pd.read_csv(\"fcs_sales_atl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge geodata with parcels and sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1. Total SFH Sales in Fulton County and Atlanta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_counts(df, label, group_col='Sale Year'):\n",
    "    return df.groupby(group_col)['PARID'].count().reset_index(name=label)\n",
    "\n",
    "dfs = []\n",
    "df_label = {\n",
    "    'Total': sales,\n",
    "    'Total Fulton Excl Atl': sales[sales[\"neighborhood\"].isna()],\n",
    "    'Valid Fulton Excl Atl': valid_sales[valid_sales[\"neighborhood\"].isna()],\n",
    "    'Invalid Fulton Excl Atl': invalid_sales[invalid_sales[\"neighborhood\"].isna()],\n",
    "    'Total Atlanta': sales_atl,\n",
    "    'Valid Atlanta': valid_atl,\n",
    "    'Invalid Atlanta': invalid_atl\n",
    "}\n",
    "for label, df in df_label.items():\n",
    "    dfs.append(calculate_counts(df, label))\n",
    "\n",
    "total_sales = reduce(lambda left, right: pd.merge(left, right, on='Sale Year'), dfs)\n",
    "\n",
    "pct_cols = [\n",
    "    \"Valid Fulton Excl Atl\",\n",
    "    \"Invalid Fulton Excl Atl\",\n",
    "    \"Valid Atlanta\",\n",
    "    \"Invalid Atlanta\"\n",
    "]\n",
    "\n",
    "pcts = total_sales[pct_cols].divide(total_sales[\"Total\"], axis=0).add_suffix(' %')\n",
    "\n",
    "total_sales = pd.concat([total_sales, pcts], axis=1)\n",
    "total_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6), gridspec_kw={'wspace': .22})\n",
    "\n",
    "utils.area_plot(\n",
    "    total_sales,\n",
    "    \"Sale Year\",\n",
    "    [\"Total\", \"Total Fulton Excl Atl\", \"Total Atlanta\"],\n",
    "    [\"Total\", \"Total Fulton (excl. ATL)\", \"Total Atlanta\"],\n",
    "    \"Sale Year\",\n",
    "    \"Total Transactions\",\n",
    "    title=\"SF Transactions in Fulton County, GA\",\n",
    "    ax=ax[0],\n",
    "    format_func=format_thousands,\n",
    "    format_tuple=(0, 1),\n",
    "    legend={\"loc\": \"upper left\", \"bbox_to_anchor\": (0, 0.97)}\n",
    ")\n",
    "\n",
    "utils.stacked_plot(\n",
    "    total_sales,\n",
    "    \"Sale Year\",\n",
    "    [\"Valid Fulton Excl Atl %\", \"Valid Atlanta %\", \"Invalid Fulton Excl Atl %\", \"Invalid Atlanta %\"],\n",
    "    [\"Typical Fulton (excl. ATL)\", \"Typical Atlanta\", \"Atypical Fulton (excl. ATL)\", \"Atypical Atlanta\"],\n",
    "    \"Sale Year\",\n",
    "    y_label=\"Proportion of Total Transactions\",\n",
    "    title=\"Proportion of SF Transactions in Fulton County, GA\",\n",
    "    ax=ax[1],\n",
    "    legend={\"loc\": \"lower right\"},\n",
    "    opacity=.85\n",
    ")\n",
    "\n",
    "ax[1].axhline(y=1, color='red', linestyle='--', alpha=.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2. Total Sales by Corporates and Table of Invalid Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want total trans in Atlanta; total and invalid broken down by corp purchases and sales\n",
    "dfs = []\n",
    "df_label = {\n",
    "    'Total Corp Trans Atl': sales_atl[(sales_atl['GRANTEE_corp_flag'] == 1) | (sales_atl['GRANTOR_corp_flag'] == 1)],\n",
    "    'Total Corp Purchases Atl': sales_atl[sales_atl['GRANTEE_corp_flag'] == 1],\n",
    "    'Total Corp Sales Atl': sales_atl[sales_atl['GRANTOR_corp_flag'] == 1],\n",
    "    'Invalid Corp Purchases Atl': invalid_atl[invalid_atl['GRANTEE_corp_flag'] == 1],\n",
    "    'Invalid Corp Sales Atl': invalid_atl[invalid_atl['GRANTOR_corp_flag'] == 1],\n",
    "    'Valid Corp Purchases Atl': valid_atl[valid_atl['GRANTEE_corp_flag'] == 1],\n",
    "    'Valid Corp Sales Atl': valid_atl[valid_atl['GRANTOR_corp_flag'] == 1]\n",
    "}\n",
    "\n",
    "for label, df in df_label.items():\n",
    "    dfs.append(calculate_counts(df, label))\n",
    "\n",
    "atlanta_breakdown = reduce(lambda left, right: pd.merge(left, right, on='Sale Year'), dfs)\n",
    "total_sales = pd.merge(total_sales, atlanta_breakdown, on='Sale Year')\n",
    "total_sales[\"Corp Trans Sum Atl\"] = total_sales[\"Total Corp Purchases Atl\"] + total_sales[\"Total Corp Sales Atl\"]\n",
    "\n",
    "pct_cols = [label for label in df_label.keys() if label != \"Total Corp Trans Atl\"]\n",
    "pcts = total_sales[pct_cols].divide(total_sales[\"Corp Trans Sum Atl\"], axis=0).add_suffix(' %')\n",
    "\n",
    "pcts_of_corp_trans = total_sales[pct_cols].divide(total_sales[\"Corp Trans Sum Atl\"], axis=0).add_suffix(' % Corp Trans')\n",
    "\n",
    "total_sales = pd.concat([total_sales, pcts, pcts_of_corp_trans], axis=1)\n",
    "total_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(14, 14), gridspec_kw={'wspace': .22, 'hspace': .3})\n",
    "\n",
    "for i, subset in enumerate([\"Total\", \"Invalid\"]):\n",
    "    name = \"Total\"\n",
    "    if i == 1:\n",
    "        loc=\"upper right\"\n",
    "    else:\n",
    "        loc=\"upper left\"\n",
    "    if subset == \"Invalid\":\n",
    "        name = \"Atypical\"\n",
    "    col_labels = [\"Total\", f\"{subset} Corp Purchases Atl\", f\"{subset} Corp Sales Atl\"]\n",
    "    utils.area_plot(\n",
    "        total_sales,\n",
    "        \"Sale Year\",\n",
    "        [f\"{subset} Atlanta\", f\"{subset} Corp Purchases Atl\", f\"{subset} Corp Sales Atl\"],\n",
    "        [x.replace(\"Invalid\", \"Atypical\") for x in col_labels],\n",
    "        title=f\"{name} SF Transcations in Atlanta, GA\",\n",
    "        x_label=\"Sale Year\",\n",
    "        y_label=\"Total Transactions\",\n",
    "        ax=ax[i][0],\n",
    "        legend={\"loc\": loc},\n",
    "        format_func=format_hundreds,\n",
    "        format_tuple=(0, 1),\n",
    "    )\n",
    "    \n",
    "    stacked_cols = [f\"{subset} Corp Purchases Atl %\", f\"{subset} Corp Sales Atl %\"]\n",
    "    trans_level = \" Corporate\"\n",
    "    if subset == \"Invalid\":\n",
    "        stacked_cols = [\n",
    "            \"Invalid Corp Purchases Atl % Corp Trans\",\n",
    "            \"Invalid Corp Sales Atl % Corp Trans\",\n",
    "            \"Valid Corp Purchases Atl % Corp Trans\",\n",
    "            \"Valid Corp Sales Atl % Corp Trans\"\n",
    "        ]\n",
    "        trans_level = \" Corporate\"\n",
    "    col_labels = [x.replace(\"Atl % Corp Trans\", \"\").replace(\"Atl %\", \"\").replace(\"Total Corp\", \"Corporate\").replace(\"Valid\", \"Typical\").replace(\"Invalid\", \"Atypical\") for x in stacked_cols]\n",
    "    if subset == \"Invalid\":\n",
    "        col_labels = [x.replace(\" Corp\", \"\").replace(\"Valid\", \"Typical\").replace(\"Invalid\", \"Atypical\") for x in col_labels]\n",
    "\n",
    "\n",
    "    utils.stacked_plot(\n",
    "        total_sales,\n",
    "        \"Sale Year\",\n",
    "        stacked_cols,\n",
    "        col_labels,\n",
    "        title=f\"Proportion of{trans_level} SF Transcations in Atlanta, GA\",\n",
    "        y_label=f\"Proportion of{trans_level} Transactions\",\n",
    "        x_label=\"Sale Year\",\n",
    "        ax=ax[i][1],\n",
    "        opacity=.65,\n",
    "        legend={\"loc\": \"lower right\"}\n",
    "    )\n",
    "    \n",
    "    ax[i][1].axhline(y=1, color='red', linestyle='--', alpha=.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3. Sales by Transaction Scale\n",
    "Note: switch with Figure 2 (e.g. this figure should come before previous) for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note includes valid sales and purchases from govt, bank\n",
    "sale_scale = pd.read_csv(\"output/sale_scale.csv\")\n",
    "sale_scale[\"TAXYR\"] = (sale_scale[\"TAXYR\"].astype(int) - 1)\n",
    "sale_scale = sale_scale.rename(columns={\"TAXYR\": \"Sale Year\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to join sale_scale to sale so we have scale of both GRANTEE and GRANTOR\n",
    "sales = sales.merge(\n",
    "    sale_scale[[\"Sale Year\", \"entity_addr\", \"total_trans_fulton\"]],\n",
    "    left_on=[\"Sale Year\", \"GRANTEE_match_addr\"], # GRANTEE scale\n",
    "    right_on=[\"Sale Year\", \"entity_addr\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"total_trans_fulton\": \"Buyer Transactions Fulton\"}).drop(\n",
    "    columns=[\"entity_addr\"]\n",
    ").merge(\n",
    "    sale_scale[[\"Sale Year\", \"entity_addr\", \"total_trans_fulton\"]],\n",
    "    left_on=[\"Sale Year\", \"GRANTOR_match_addr\"], # GRANTOR scale\n",
    "    right_on=[\"Sale Year\", \"entity_addr\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"total_trans_fulton\": \"Seller Transactions Fulton\"}).drop(\n",
    "    columns=[\"entity_addr\"]\n",
    ")\n",
    "\n",
    "for col in [\"Buyer Transactions Fulton\", \"Seller Transactions Fulton\"]:\n",
    "    sales[col] = sales[col].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head(5)[[\"Sale Year\", \"GRANTEE_match_addr\", \"GRANTOR_match_addr\", \"Buyer Transactions Fulton\", \"Seller Transactions Fulton\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[\"Buyer Scale\"] = sales[\n",
    "    sales[\"GRANTEE_corp_flag\"] == 1\n",
    "][\"Buyer Transactions Fulton\"].apply(classify_corp)\n",
    "\n",
    "sales[\"Seller Scale\"] = sales[\n",
    "    sales[\"GRANTOR_corp_flag\"] == 1\n",
    "][\"Seller Transactions Fulton\"].apply(classify_corp)\n",
    "\n",
    "for col in [\"Buyer Scale\", \"Seller Scale\"]:\n",
    "    sales[col] = sales[col].fillna(0).astype(int)\n",
    "\n",
    "sales_atl = sales[sales[\"neighborhood\"].notna()]\n",
    "# Add scale metrics to total sales table; e.g. count of sales by scale\n",
    "# so we want to sum total sales, total valid sales where scale is 1, 2, 3 for each year\n",
    "dfs = [total_sales] + []\n",
    "\n",
    "scales = {\n",
    "    0: \"Individual\",\n",
    "    1: \"Small Corporate\",\n",
    "    2: \"Medium Corporate\",\n",
    "    3: \"Large Corporate\",\n",
    "}\n",
    "\n",
    "names = {\n",
    "    \"Seller\": \"Sales\",\n",
    "    \"Buyer\": \"Purchases\"\n",
    "}\n",
    "\n",
    "for trans_type in [\"Buyer\", \"Seller\"]:\n",
    "    for i in scales.keys():\n",
    "        df = calculate_counts(sales_atl[sales_atl[f\"{trans_type} Scale\"] == i], f\"{scales[i]} {names[trans_type]} Atlanta\")\n",
    "        dfs.append(df)\n",
    "        \n",
    "total_sales = reduce(lambda left, right: pd.merge(left, right, on='Sale Year'), dfs)\n",
    "total_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_cols = [\n",
    "    \"Individual Purchases Atlanta\",\n",
    "    \"Small Corporate Purchases Atlanta\",\n",
    "    \"Medium Corporate Purchases Atlanta\",\n",
    "    \"Large Corporate Purchases Atlanta\",\n",
    "    \"Individual Sales Atlanta\",\n",
    "    \"Small Corporate Sales Atlanta\",\n",
    "    \"Medium Corporate Sales Atlanta\",\n",
    "    \"Large Corporate Sales Atlanta\"\n",
    "]\n",
    "\n",
    "pct_by_type = total_sales[pct_cols].divide(total_sales[\"Total Atlanta\"], axis=0).add_suffix(' %')\n",
    "\n",
    "total_sales = pd.concat([total_sales, pct_by_type], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12), gridspec_kw={'wspace': .22, 'hspace': .3})\n",
    "\n",
    "for i, type in enumerate([\"Purchases\", \"Sales\"]):\n",
    "    utils.area_plot(\n",
    "        df=total_sales,\n",
    "        x=\"Sale Year\",\n",
    "        cols=[\"Total Atlanta\"] + [f\"{scales[i]} {type} Atlanta\" for i in scales.keys()],\n",
    "        labels=[f\"Total {type}\"] + [f\"{scales[i]} {type}\".replace(\"Corporate\", \"Corp\") for i in scales.keys()],\n",
    "        x_label=\"Sale Year\",\n",
    "        y_label=f\"Total {type}\",\n",
    "        title=f\"SF {type} in Atlanta, GA\",\n",
    "        ax=ax[i][0],\n",
    "        format_func=format_thousands,\n",
    "        format_tuple=(0, 1),\n",
    "        legend={\"loc\": \"upper left\"}\n",
    "    )\n",
    "    \n",
    "    utils.stacked_plot(\n",
    "        df=total_sales,\n",
    "        x=\"Sale Year\",\n",
    "        cols=[f\"{scales[i]} {type} Atlanta %\" for i in scales.keys()],\n",
    "        labels=[f\"{scales[i]} {type}\".replace(\"Corporate\", \"Corp\") for i in scales.keys()],\n",
    "        x_label=\"Sale Year\",\n",
    "        y_label=f\"Proportion of {type}\",\n",
    "        title=f\"Proportion of SF {type} in Atlanta, GA\",\n",
    "        ax=ax[i][1],\n",
    "        opacity=.65,\n",
    "        legend={\"loc\": \"lower right\"}\n",
    "    )\n",
    "    \n",
    "    ax[i][1].axhline(y=1, color='red', linestyle='--', alpha=.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4. Ownership of SFH Rentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_scale = pd.read_csv(\"output/owner_scale.csv\")\n",
    "owner_scale.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest = digest.merge(\n",
    "    owner_scale[[\"TAXYR\", \"owner_addr\", \"count_owned_fulton_yr\"]],\n",
    "    on=[\"TAXYR\", \"owner_addr\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "digest[\"owner scale\"] = digest[digest[\"own_corp_flag\"] == 1][\"count_owned_fulton_yr\"].apply(classify_corp)\n",
    "digest[\"owner scale\"] = digest[\"owner scale\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals = digest[digest[\"rental_flag\"] == 1]\n",
    "rentals_atl = rentals[rentals[\"neighborhood\"].notna()]\n",
    "\n",
    "num_rentals_fulton = calculate_counts(rentals, \"Rentals Fulton\", group_col=\"TAXYR\")\n",
    "num_rentals_atl = calculate_counts(rentals_atl, \"Rentals Atlanta\", group_col=\"TAXYR\")\n",
    "\n",
    "dfs = [num_rentals_fulton] + [num_rentals_atl] + []\n",
    "\n",
    "for scale in scales.keys():\n",
    "    df = calculate_counts(rentals[rentals[\"owner scale\"] == scale], f\"{scales[scale]} Fulton\", group_col=\"TAXYR\")\n",
    "    dfs.append(df)\n",
    "    \n",
    "    df = calculate_counts(rentals_atl[rentals_atl[\"owner scale\"] == scale], f\"{scales[scale]} Atlanta\", group_col=\"TAXYR\")\n",
    "    dfs.append(df)\n",
    "\n",
    "rental_summary = reduce(lambda left, right: pd.merge(left, right, on='TAXYR'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [rental_summary] + []\n",
    "\n",
    "pct_cols_fulton = [\n",
    "    \"Individual Fulton\",\n",
    "    \"Small Corporate Fulton\",\n",
    "    \"Medium Corporate Fulton\",\n",
    "    \"Large Corporate Fulton\",\n",
    "]\n",
    "pct_cols_atl = [\n",
    "    \"Individual Atlanta\",\n",
    "    \"Small Corporate Atlanta\",\n",
    "    \"Medium Corporate Atlanta\",\n",
    "    \"Large Corporate Atlanta\"\n",
    "]\n",
    "\n",
    "pct_rental_type_fulton = rental_summary[pct_cols_fulton].divide(rental_summary[\"Rentals Fulton\"], axis=0).add_suffix(' %')\n",
    "pct_rental_type_atl = rental_summary[pct_cols_atl].divide(rental_summary[\"Rentals Atlanta\"], axis=0).add_suffix(' %')\n",
    "\n",
    "dfs = dfs + [pct_rental_type_fulton, pct_rental_type_atl]\n",
    "\n",
    "rental_summary = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), dfs)\n",
    "rental_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6), gridspec_kw={'wspace': .22, 'hspace': .3})\n",
    "\n",
    "area = \"Atlanta\"\n",
    "\n",
    "utils.area_plot(\n",
    "    rental_summary,\n",
    "    \"TAXYR\",\n",
    "    [f\"Rentals {area}\", f\"Individual {area}\", f\"Small Corporate {area}\", f\"Medium Corporate {area}\", f\"Large Corporate {area}\"],\n",
    "    [f\"Total Rentals\", f\"Individual\", f\"Small Corporate\", f\"Medium Corporate\", f\"Large Corporate\"],\n",
    "    \"Tax Year\",\n",
    "    \"Total SFRs\",\n",
    "    f\"SFRs by Ownership in {area}, GA\",\n",
    "    legend={\"loc\": \"center left\"},\n",
    "    ax=ax[0],\n",
    "    format_func=format_thousands,\n",
    "    format_tuple=(0, 1)\n",
    ")\n",
    "\n",
    "utils.stacked_plot(\n",
    "    rental_summary,\n",
    "    \"TAXYR\",\n",
    "    [f\"Individual {area} %\", f\"Small Corporate {area} %\", f\"Medium Corporate {area} %\", f\"Large Corporate {area} %\"],\n",
    "    [f\"Individual\", f\"Small Corporate\", f\"Medium Corporate\", f\"Large Corporate\"],\n",
    "    \"Tax Year\",\n",
    "    y_label=\"Proportion of SFRs\",\n",
    "    title=f\"Proportion of SFRs in {area} by Ownership\",\n",
    "    legend={\"loc\": \"lower left\"},\n",
    "    ax=ax[1],\n",
    "    opacity=.65\n",
    ")\n",
    "\n",
    "ax[1].axhline(y=1, color='red', linestyle='--', alpha=.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5. Neighborhood characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsa_geo.rename(columns={\"NEIGHBORHO\": \"neighborhood\"}, inplace=True)\n",
    "nsa_geo = nsa_geo[['STATISTICA', 'neighborhood', 'geometry']]\n",
    "\n",
    "nsa_stats.rename(columns={\"Details\": \"neighborhood\"}, inplace=True)\n",
    "nsa_stats = nsa_stats[[\n",
    "    \"GEOID\",\n",
    "    \"neighborhood\",\n",
    "    \"Median household income 2021\",\n",
    "    \"% Not Hispanic Black or African American alone 2021\",\n",
    "]]\n",
    "\n",
    "nsa_geo = nsa_geo.merge(\n",
    "    nsa_stats,\n",
    "    on=\"neighborhood\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dropped_nbhds = [\n",
    "    'East Lake, The Villages at East Lake',\n",
    "    'Edgewood',\n",
    "    'Candler Park, Druid Hills',\n",
    "    'Kirkwood',\n",
    "    'Lake Claire',\n",
    "    'East Atlanta',\n",
    "    'Airport'\n",
    "]\n",
    "\n",
    "nsa_geo = nsa_geo[~nsa_geo[\"neighborhood\"].isin(dropped_nbhds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "hues = [\n",
    "    \"Median household income 2021\",\n",
    "    \"% Not Hispanic Black or African American alone 2021\",\n",
    "]\n",
    "titles = [\n",
    "    \"Median Household Income (2021, ACS)\",\n",
    "    \"Percent Non-Hispanic Black (2021, ACS)\",\n",
    "]\n",
    "highlight = [\n",
    "    \"Castleberry Hill, Downtown\",\n",
    "    \"Buckhead Heights, Lenox, Ridgedale Park\"\n",
    "    #\"Kingswood, Mt. Paran/Northside, Mt. Paran Parkway, Randall Mill, West Paces Ferry/Northside, Whitewater Creek\",\n",
    "    #\"Chastain Park, Tuxedo Park\",\n",
    "    #'Adair Park, Pittsburgh',\n",
    "    #'Lakewood, Leila Valley, Norwood Manor, Rebel Valley Forest',\n",
    "]\n",
    "\n",
    "utils.map(\n",
    "    nsa_geo,\n",
    "    color=hues[0],\n",
    "    title=titles[0],\n",
    "    nbhd_df=nsa_geo,\n",
    "    ax=axes[0],\n",
    "    highlight=highlight,\n",
    "    format_func=format_thousands\n",
    ")\n",
    "\n",
    "utils.map(\n",
    "    nsa_geo,\n",
    "    color=hues[1],\n",
    "    title=titles[1],\n",
    "    nbhd_df=nsa_geo,\n",
    "    ax=axes[1],\n",
    "    highlight=highlight,\n",
    "    format_func=format_percent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 6. Sales per SFH Parcel, Proportion Valid SFH Sales by Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "dfs.append(calculate_counts(sales_atl, \"Total Sales\", group_col=\"neighborhood\"))\n",
    "dfs.append(calculate_counts(invalid_atl, \"Invalid Sales\", group_col=\"neighborhood\"))\n",
    "dfs.append(calculate_counts(digest_atl[digest_atl[\"TAXYR\"] == 2022], \"Total Parcels 2022\", group_col=\"neighborhood\"))\n",
    "\n",
    "atl_sales_summary = reduce(lambda left, right: pd.merge(left, right, on='neighborhood'), dfs)\n",
    "\n",
    "atl_sales_summary[\"Sales Per Parcel\"] = atl_sales_summary[\"Total Sales\"].divide(atl_sales_summary[\"Total Parcels 2022\"], axis=0)\n",
    "atl_sales_summary[\"Percent Invalid Sales\"] = atl_sales_summary[\"Invalid Sales\"].divide(atl_sales_summary[\"Total Sales\"], axis=0)\n",
    "atl_sales_summary = atl_sales_summary.merge(nsa_geo[[\"neighborhood\", \"geometry\"]], on=\"neighborhood\", how=\"left\")\n",
    "\n",
    "atl_sales_summary.sort_values(\"Sales Per Parcel\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "atl_sales_summary = atl_sales_summary.fillna(0)\n",
    "#outliers_1 = [\"Georgia Tech, Marietta Street Artery\", \"Lindbergh/Morosgo\"]\n",
    "highlight1 = [\n",
    "    'Carver Hills, Rockdale, Scotts Crossing, West Highlands',\n",
    "    'Lindbergh/Morosgo',\n",
    "    'Adair Park, Pittsburgh',\n",
    "]\n",
    "highlight2 = [\n",
    "    \"English Avenue\",\n",
    "    'Atlanta University Center, The Villages at Castleberry Hill',\n",
    "    'Center Hill, Harvel Homes Community',\n",
    "]\n",
    "utils.map(\n",
    "    atl_sales_summary,\n",
    "    color=\"Sales Per Parcel\",\n",
    "    title=\"Sales per SF Parcel by Neighborhood\",\n",
    "    nbhd_df=nsa_geo,\n",
    "    ax=ax[0],\n",
    "    log=True,\n",
    "    highlight=highlight1,\n",
    ")\n",
    "\n",
    "utils.map(\n",
    "    atl_sales_summary,\n",
    "    color=\"Percent Invalid Sales\",\n",
    "    title=\"Proportion Atypical SF Sales by Neighborhood\",\n",
    "    nbhd_df=nsa_geo,\n",
    "    ax=ax[1],\n",
    "    highlight=highlight2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7. Percent of Sales at each Transaction Scale by Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_atl_2022 = digest[(digest[\"TAXYR\"] == 2022) & (digest[\"neighborhood\"].notna())]\n",
    "\n",
    "dfs = [calculate_counts(digest_atl_2022, \"Total Parcels 2022\", group_col=\"neighborhood\")] + []\n",
    "\n",
    "for scale in scales:\n",
    "    df = calculate_counts(\n",
    "        digest_atl_2022[digest_atl_2022[\"owner scale\"] == scale],\n",
    "        f\"{scales[scale]}\",\n",
    "        group_col=\"neighborhood\"\n",
    "    )\n",
    "    \n",
    "    dfs.append(df)\n",
    "    \n",
    "ownership_2022 = reduce(lambda left, right: pd.merge(left, right, on=\"neighborhood\", how=\"left\"), dfs)\n",
    "ownership_2022 = ownership_2022.fillna(0)\n",
    "\n",
    "pct_cols = [f\"{scales[i]}\" for i in scales.keys()]\n",
    "pct_by_owner = ownership_2022[pct_cols].divide(ownership_2022[\"Total Parcels 2022\"], axis=0).add_suffix(' %')\n",
    "ownership_2022 = pd.concat([ownership_2022, pct_by_owner], axis=1)\n",
    "\n",
    "ownership_2022 = ownership_2022.merge(\n",
    "    nsa_geo[[\"neighborhood\", \"geometry\"]],\n",
    "    on=\"neighborhood\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "ownership_2022.sort_values(by=\"Large Corporate %\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of parcels owned by individuals\n",
    "digest_atl_2022[digest_atl_2022[\"own_corp_flag\"] != 1][\"PARID\"].count() / digest_atl_2022[\"PARID\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make 0 grayed out (could just plot a map underneath)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12), gridspec_kw={'wspace': .22, 'hspace': .3})\n",
    "\n",
    "highlight = [\n",
    "    [\n",
    "        \"English Avenue\",\n",
    "        'Lakewood, Leila Valley, Norwood Manor, Rebel Valley Forest',\n",
    "    ],\n",
    "    [\n",
    "        'Atlanta University Center, The Villages at Castleberry Hill',\n",
    "        'Lakewood, Leila Valley, Norwood Manor, Rebel Valley Forest',\n",
    "    ],\n",
    "    [\n",
    "        \"English Avenue\",\n",
    "        \"Mechanicsville\",\n",
    "    ],\n",
    "    [\n",
    "        \"South River Gardens\",\n",
    "         'Baker Hills, Bakers Ferry, Boulder Park, Fairburn Road/Wisteria Lane, Ridgecrest Forest, Wildwood (NPU-H), Wilson Mill Meadows, Wisteria Gardens',\n",
    "    ]\n",
    "]\n",
    "\n",
    "log=False\n",
    "for i, scale in enumerate(scales.values()):\n",
    "    # neighborhoods we already excluded\n",
    "    if i in [1, 2, 3]:\n",
    "        log=True\n",
    "    else:\n",
    "        log=False\n",
    "    utils.map(\n",
    "        ownership_2022[~ownership_2022[\"neighborhood\"].str.contains(\"Downtown\")],\n",
    "        color=f\"{scale} %\",\n",
    "        title=f\"Proportion Owned by {scale}s, 2022\",\n",
    "        nbhd_df=nsa_geo,\n",
    "        ax=ax.flatten()[i],\n",
    "        log=log,\n",
    "        highlight=highlight[i],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 8. Percent of Sales by Sale Type Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [] + [atl_sales_summary]\n",
    "\n",
    "sale_types = [\"corp_bought_ind\", \"corp_sold_ind\", \"ind_to_ind\", \"corp_to_corp\"]\n",
    "for sale in sale_types:\n",
    "    dfs.append(pd.DataFrame(sales_atl[\n",
    "        sales_atl[sale] == 1\n",
    "    ].groupby(\"neighborhood\")[\"PARID\"].count()).rename(columns={\"PARID\": f\"Total {sale}\"}).reset_index())\n",
    "    \n",
    "atl_sales_summary = reduce(lambda left, right: pd.merge(left, right, on=\"neighborhood\", how=\"left\"), dfs)\n",
    "\n",
    "atl_sales_summary = atl_sales_summary.fillna(0)\n",
    "atl_sales_summary['Percent corp_bought_ind'] = atl_sales_summary['Total corp_bought_ind'] / atl_sales_summary['Total Sales']\n",
    "atl_sales_summary['Percent corp_sold_ind'] = atl_sales_summary['Total corp_sold_ind'] / atl_sales_summary['Total Sales']\n",
    "atl_sales_summary['Percent ind_to_ind'] = atl_sales_summary['Total ind_to_ind'] / atl_sales_summary['Total Sales']\n",
    "atl_sales_summary['Percent corp_to_corp'] = atl_sales_summary['Total corp_to_corp'] / atl_sales_summary['Total Sales']\n",
    "atl_sales_summary.sort_values(by=\"Percent corp_bought_ind\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12), gridspec_kw={'wspace': .22, 'hspace': .3})\n",
    "\n",
    "highlight1 = [\n",
    "    \"Hammond Park\",\n",
    "    'Bankhead Courts, Bankhead/Bolton, Carroll Heights, Fairburn Heights, Old Gordon',\n",
    "]\n",
    "\n",
    "highlight2 = [\n",
    "    \"Lindbergh/Morosgo\",\n",
    "    \"Carver Hills, Rockdale, Scotts Crossing, West Highlands\",\n",
    "    \"Fairburn Mays, Mays\",\n",
    "]\n",
    "\n",
    "highlight3 = [\n",
    "    'Collier Hills, Collier Hills North, Colonial Homes',\n",
    "    'Grant Park, Oakland',\n",
    "]\n",
    "\n",
    "highlight4 = [\n",
    "    \"Almond Park, Carey Park\",\n",
    "    \"Lakewood, Leila Valley, Norwood Manor, Rebel Valley Forest\",\n",
    "]\n",
    "\n",
    "utils.map(\n",
    "    atl_sales_summary,\n",
    "    color=\"Percent corp_bought_ind\",\n",
    "    title=\"Proportion of Sales (Corporate Purchase from Individual)\",\n",
    "    nbhd_df=nsa_geo,\n",
    "    ax=ax[0][0],\n",
    "    log=True,\n",
    "    highlight=highlight1\n",
    ")\n",
    "utils.map(\n",
    "    atl_sales_summary,\n",
    "    color=\"Percent corp_sold_ind\",\n",
    "    title=\"Proportion of Sales (Corporate Sale to Individuals)\",\n",
    "    nbhd_df=nsa_geo,\n",
    "    ax=ax[0][1],\n",
    "    log=True,\n",
    "    highlight=highlight2\n",
    ")\n",
    "utils.map(\n",
    "    atl_sales_summary,\n",
    "    color=\"Percent ind_to_ind\",\n",
    "    title=\"Proportion of Sales (Individual to Individual)\",\n",
    "    nbhd_df=nsa_geo,\n",
    "    ax=ax[1][0],\n",
    "    highlight=highlight3\n",
    ")\n",
    "utils.map(\n",
    "    atl_sales_summary,\n",
    "    color=\"Percent corp_to_corp\",\n",
    "    title=\"Proportion of Sales (Corporate to Corporate)\",\n",
    "    nbhd_df=nsa_geo,\n",
    "    ax=ax[1][1],\n",
    "    highlight=highlight4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 9. Graph of Majority-Black vs Other Neighborhoods by Corporate Purchases and Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate neighborhoods into majority Black and other with mb_flag\n",
    "mb_neighborhoods = nsa_geo[\n",
    "    nsa_geo[\"% Not Hispanic Black or African American alone 2021\"] >= 50\n",
    "][\"neighborhood\"].unique()\n",
    "\n",
    "sales[\"mb_flag\"] = sales[\"neighborhood\"].apply(lambda x: 1 if x in mb_neighborhoods else 0)\n",
    "# Agg all sales on mb_flag, calculate total transcations, number where buyer is corp,\n",
    "# seller is corp, buyer is corp and seller is corp, then calculate percent\n",
    "total_sales = calculate_counts(sales, \"total_sales\", group_col=[\"mb_flag\", \"Sale Year\"])\n",
    "\n",
    "all_corp_trans = sales[(sales[\"GRANTEE_corp_flag\"] == 1) | (sales[\"GRANTOR_corp_flag\"] == 1)]\n",
    "corp_trans = calculate_counts(all_corp_trans, \"corp_trans\", group_col=[\"mb_flag\", \"Sale Year\"])\n",
    "\n",
    "mb_summary = total_sales.merge(corp_trans, on=[\"mb_flag\", \"Sale Year\"], how=\"left\")\n",
    "mb_summary[\"% corp\"] = mb_summary[\"corp_trans\"] / mb_summary[\"total_sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add in corp to corp / invalid\n",
    "# TODO for each point, put absolute numbers\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "utils.area_plot(\n",
    "    mb_summary[mb_summary[\"mb_flag\"] == 0],\n",
    "    \"Sale Year\",\n",
    "    [\"% corp\"],\n",
    "    [\"Non-Majority Black Neighborhood\"],\n",
    "    \"Sale Year\",\n",
    "    title=\"Proportion of Transactions Involving a Corporate Entity\",\n",
    "    ax=ax,\n",
    "    legend={\"loc\": \"lower left\"},\n",
    "    ann=\"total_sales\"\n",
    ")\n",
    "\n",
    "utils.area_plot(\n",
    "    mb_summary[mb_summary[\"mb_flag\"] == 1],\n",
    "    \"Sale Year\",\n",
    "    [\"% corp\"],\n",
    "    [\"Majority Black Neighborhood\"],\n",
    "    \"Sale Year\",\n",
    "    \"Proportion of Transactions\",\n",
    "    ax=ax,\n",
    "    legend={\"loc\": \"lower left\"},\n",
    "    ann=\"total_sales\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Equity Loss Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation by Year\n",
    "Drop outliers where sale_diff is too extreme (multi-parcel sales with properties outside Fulton most likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sale_diff = fmv_adj - price_adj\n",
    "sales[\"sale_diff_adj\"] = sales[\"fmv_adj\"] - sales[\"price_adj\"]\n",
    "sales[\"sale_diff\"] = sales[\"FAIR MARKET VALUE\"] - sales[\"sales_price\"]\n",
    "original_sales = sales.copy(deep=True)\n",
    "\n",
    "# Calculate the 99th percentile of sale_diff\n",
    "top_threshold = sales[\"sale_diff_adj\"].quantile(0.99)\n",
    "bottom_threshold = sales[\"sale_diff_adj\"].quantile(0.01)\n",
    "\n",
    "# Filter out the rows where sale_diff is greater than the threshold\n",
    "sales = sales[\n",
    "    (sales[\"sale_diff_adj\"] < top_threshold)\n",
    "    & (sales[\"sale_diff_adj\"] > bottom_threshold)\n",
    "]\n",
    "dropped_sales = original_sales[(original_sales[\"sale_diff_adj\"] >= top_threshold) | (original_sales[\"sale_diff_adj\"] <= bottom_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg(df, label, agg_col, group_col='Sale Year'):\n",
    "    if isinstance(group_col, list):\n",
    "        return df.groupby(by=group_col)[agg_col].sum().reset_index(name=label)\n",
    "    else:\n",
    "        return df.groupby(by=[group_col])[agg_col].sum().reset_index(name=label)\n",
    "    \n",
    "# sale loss: aggregate sale_diff where corp_sold_ind = 1\n",
    "# positive indicates sold for below FMV, company contributed to neighborhood equity\n",
    "dfs = []\n",
    "atl_sales = sales[sales[\"neighborhood\"].notna()]\n",
    "\n",
    "dfs.append(agg(atl_sales[atl_sales[\"corp_sold_ind\"] == 1], \"sale_loss_adj\", \"sale_diff_adj\", group_col=[\"TAXYR\", \"neighborhood\"]))\n",
    "dfs.append(agg(atl_sales[atl_sales[\"corp_bought_ind\"] == 1], \"purchase_loss_adj\", \"sale_diff_adj\", group_col=[\"TAXYR\", \"neighborhood\"]))\n",
    "\n",
    "dfs.append(agg(atl_sales[atl_sales[\"corp_sold_ind\"] == 1], \"sale_loss\", \"sale_diff\", group_col=[\"TAXYR\", \"neighborhood\"]))\n",
    "dfs.append(agg(atl_sales[atl_sales[\"corp_bought_ind\"] == 1], \"purchase_loss\", \"sale_diff\", group_col=[\"TAXYR\", \"neighborhood\"]))\n",
    "\n",
    "dfs.append(agg(atl_sales[(atl_sales[\"corp_sold_ind\"] == 1) & (atl_sales[\"GRANTOR_match\"].str.contains(\"CHARIS\"))], \"sale_loss_adj_charis\", \"sale_diff_adj\", group_col=[\"TAXYR\", \"neighborhood\"]))\n",
    "dfs.append(agg(atl_sales[(atl_sales[\"corp_bought_ind\"] == 1) & (atl_sales[\"GRANTEE_match\"].str.contains(\"CHARIS\"))], \"purchase_loss_adj_charis\", \"sale_diff_adj\", group_col=[\"TAXYR\", \"neighborhood\"]))\n",
    "\n",
    "# rental loss: sum of aprtot_adj * .005 * 12 where rental_flag = 1 and own_corp_flag = 1\n",
    "digest[\"rental_value_adj\"] = digest[\"Aprtot_adj\"] * .005 * 12\n",
    "digest[\"rental_value\"] = digest[\"Aprtot\"] * .005 * 12\n",
    "\n",
    "# ignore 2022 rental loss, since sale data ends at 2021\n",
    "rental_loss_digest = digest[\n",
    "    (digest[\"neighborhood\"].notna())\n",
    "    & (digest[\"rental_flag\"] == 1)\n",
    "    & (digest[\"own_corp_flag\"] == 1)\n",
    "    & (digest[\"TAXYR\"] < 2022)\n",
    "]\n",
    "dfs.append(agg(rental_loss_digest, \"rental_loss_adj\", \"rental_value_adj\", group_col=[\"TAXYR\", \"neighborhood\"]))\n",
    "dfs.append(agg(rental_loss_digest[rental_loss_digest[\"Own1\"].str.contains(\"CHARIS\")], \"rental_loss_adj_charis\", \"rental_value_adj\", group_col=[\"TAXYR\", \"neighborhood\"]))\n",
    "dfs.append(agg(rental_loss_digest, \"rental_loss\", \"rental_value\", group_col=[\"TAXYR\", \"neighborhood\"]))\n",
    "\n",
    "equity_summary = reduce(lambda left, right: pd.merge(left, right, on=[\"TAXYR\", \"neighborhood\"], how=\"left\"), dfs)\n",
    "\n",
    "# flip sign so negative is contributing, positive is loss\n",
    "equity_summary[\"sale_loss_adj\"] = -equity_summary[\"sale_loss_adj\"]\n",
    "equity_summary[\"total_loss_adj\"] = equity_summary[\"sale_loss_adj\"] + equity_summary[\"purchase_loss_adj\"] + equity_summary[\"rental_loss_adj\"]\n",
    "\n",
    "equity_summary[\"sale_loss\"] = -equity_summary[\"sale_loss\"]\n",
    "equity_summary[\"total_loss\"] = equity_summary[\"sale_loss\"] + equity_summary[\"purchase_loss\"] + equity_summary[\"rental_loss\"]\n",
    "\n",
    "equity_summary = equity_summary.merge(\n",
    "    nsa_geo,\n",
    "    on=\"neighborhood\",\n",
    "    how=\"left\"\n",
    ")\n",
    "equity_summary.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_summary = equity_summary.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_data = pd.read_csv(\"data/atl_nsa_households.csv\", skiprows=1).rename(columns={\"Details\": \"neighborhood\"})\n",
    "household_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agg on parcel level (e.g. sum sale_diff across all sales for each parcel), map\n",
    "\n",
    "loss_per_parcel = atl_sales.groupby(\"PARID\")[\"sale_diff_adj\"].sum().reset_index(name=\"total_sale_diff_adj\").sort_values(by=\"total_sale_diff_adj\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_parcel = loss_per_parcel.merge(digest_atl[digest_atl[\"TAXYR\"] == 2022], on=\"PARID\", how=\"inner\")\n",
    "from shapely import wkt\n",
    "\n",
    "loss_per_parcel['geometry'] = loss_per_parcel['geometry'].apply(wkt.loads)\n",
    "loss_per_parcel = gpd.GeoDataFrame(loss_per_parcel, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_parcel.explore(tiles=\"Stamen Toner\", column=\"total_sale_diff_adj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_parcel.sort_values(by=\"total_sale_diff_adj\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "dfs.append(agg(equity_summary, \"sale_loss_adj\", \"sale_loss_adj\", group_col=[\"neighborhood\"]))\n",
    "dfs.append(agg(equity_summary, \"purchase_loss_adj\", \"purchase_loss_adj\", group_col=[\"neighborhood\"]))\n",
    "dfs.append(agg(equity_summary, \"sale_loss_adj_charis\", \"sale_loss_adj_charis\", group_col=[\"neighborhood\"]))\n",
    "dfs.append(agg(equity_summary, \"purchase_loss_adj_charis\", \"purchase_loss_adj_charis\", group_col=[\"neighborhood\"]))\n",
    "dfs.append(agg(equity_summary, \"rental_loss_adj\", \"rental_loss_adj\", group_col=[\"neighborhood\"]))\n",
    "\n",
    "dfs.append(agg(equity_summary, \"sale_loss\", \"sale_loss\", group_col=[\"neighborhood\"]))\n",
    "dfs.append(agg(equity_summary, \"purchase_loss\", \"purchase_loss\", group_col=[\"neighborhood\"]))\n",
    "dfs.append(agg(equity_summary, \"rental_loss\", \"rental_loss\", group_col=[\"neighborhood\"]))\n",
    "\n",
    "total_equity = reduce(lambda left, right: pd.merge(left, right, on=\"neighborhood\", how=\"left\"), dfs)\n",
    "total_equity = total_equity.fillna(0)\n",
    "\n",
    "total_equity[\"total_loss_adj\"] = total_equity[\"sale_loss_adj\"] + total_equity[\"purchase_loss_adj\"] + total_equity[\"rental_loss_adj\"]\n",
    "total_equity[\"total_loss\"] = total_equity[\"sale_loss\"] + total_equity[\"purchase_loss\"] + total_equity[\"rental_loss\"]\n",
    "\n",
    "total_equity = total_equity.merge(\n",
    "    nsa_geo,\n",
    "    on=\"neighborhood\",\n",
    "    how=\"left\"\n",
    ").merge(\n",
    "    household_data[[\"neighborhood\", \"Average household size 2021\", \"# Total households 2021\"]],\n",
    "    on=\"neighborhood\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_equity[total_equity[\"rental_loss_adj\"] > 0].sort_values(\"total_loss_adj\", ascending=False).head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl_sales[(atl_sales[\"GRANTOR_match\"].str.contains(\"CHARIS\")) & (atl_sales[\"neighborhood\"] == \"Thomasville Heights\")].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_equity[\"total_loss_adj\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{total_equity[\"total_loss_adj\"].sum():,.0f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest[\n",
    "    (digest[\"neighborhood\"].str.contains(\"Kingswood\"))\n",
    "    & (digest[\"rental_flag\"] == 1)\n",
    "    & (digest[\"own_corp_flag\"] == 1)\n",
    "][[\"PARID\", \"Situs Adrno\", \"Situs Adrstr\", \"owner_addr\", \"Own1\", \"Aprtot_adj\", \"rental_value\", \"TAXYR\"]].sort_values(\"rental_value\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_equity.sort_values(by=\"total_loss_adj\", ascending=True).head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10. Purchase, Sale, and Rental Equity Loss Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12), gridspec_kw={'wspace': .22, 'hspace': .3})\n",
    "\n",
    "names = {\n",
    "    \"sale_loss_adj\": \"Sale\",\n",
    "    \"purchase_loss_adj\": \"Purchase\",\n",
    "    \"rental_loss_adj\": \"Rental\",\n",
    "    \"total_loss_adj\": \"Total\"\n",
    "}\n",
    "\n",
    "j = 0\n",
    "i = 0\n",
    "\n",
    "highlight = [\n",
    "    [\n",
    "        \"Bush Moutain, Oakland City\",\n",
    "        \"Adair Park, Pittsburgh\",\n",
    "        \"Sylvan Hills\"\n",
    "    ],\n",
    "    [\n",
    "        \"Morningside/Lenox Park\",\n",
    "        \"Ormewood Park\",\n",
    "        \"Grant Park, Oakland\"\n",
    "    ],\n",
    "    [\n",
    "        \"Kingswood, Mt. Paran/Northside, Mt. Paran Parkway, Randall Mill, West Paces Ferry/Northside, Whitewater Creek\",\n",
    "        \"Chastain Park, Tuxedo Park\",\n",
    "    ],\n",
    "    [\n",
    "        \"Kingswood, Mt. Paran/Northside, Mt. Paran Parkway, Randall Mill, West Paces Ferry/Northside, Whitewater Creek\",\n",
    "        \"Chastain Park, Tuxedo Park\",\n",
    "        \"Ormewood Park\",\n",
    "        \"Grant Park, Oakland\",\n",
    "        \"Adair Park, Pittsburgh\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "h = 0\n",
    "for color in [\"purchase_loss_adj\", \"sale_loss_adj\", \"rental_loss_adj\", \"total_loss_adj\"]:\n",
    "    utils.map(\n",
    "        total_equity,\n",
    "        color=color,\n",
    "        title=f\"{names[color]} Loss by Neighborhood\",\n",
    "        nbhd_df=nsa_geo,\n",
    "        ax=ax[i][j],\n",
    "        format_func=format_millions,\n",
    "        highlight=highlight[h],\n",
    "    )\n",
    "    h += 1\n",
    "    j += 1\n",
    "    if j == 2:\n",
    "        j = 0\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11. Equity Burden Map and Boxplot of Majority-Black vs Other Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_year_nsa = pd.read_csv(\"data/atl_nsa_by_year.csv\").rename(\n",
    "    columns={\n",
    "        \"tAggregateHHInc_e\": \"income_generated\"\n",
    "    }\n",
    ")\n",
    "per_year_nsa[\"NSA\"] = per_year_nsa[\"NSA\"].astype(str)\n",
    "per_year_nsa[\"Year\"] = per_year_nsa[\"Year\"].astype(int)\n",
    "equity_summary[\"GEOID\"] = equity_summary[\"GEOID\"].astype(str)\n",
    "per_year_nsa[\"income_generated\"] = per_year_nsa.groupby(\"NSA\")[\"income_generated\"].ffill()\n",
    "per_year_nsa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_summary = equity_summary.merge(\n",
    "    per_year_nsa,\n",
    "    left_on=[\"GEOID\", \"TAXYR\"],\n",
    "    right_on=[\"NSA\", \"Year\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_summary.groupby(\"neighborhood\")[\"total_loss\"].sum().sort_values(ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_summary[\"equity_burden\"] = equity_summary[\"total_loss\"] / equity_summary[\"income_generated\"]\n",
    "equity_summary.sort_values(by=\"equity_burden\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = equity_summary.groupby(\"neighborhood\")[\"total_loss\"].sum()\n",
    "income = equity_summary.groupby(\"neighborhood\")[\"income_generated\"].sum()\n",
    "\n",
    "equity_burden = pd.concat([loss, income], axis=1).reset_index()\n",
    "equity_burden[\"equity_burden\"] = equity_burden[\"total_loss\"] / equity_burden[\"income_generated\"]\n",
    "equity_burden = equity_burden.merge(\n",
    "    nsa_geo,\n",
    "    on=\"neighborhood\",\n",
    "    how=\"left\"\n",
    ")\n",
    "equity_burden[\"equity_burden\"] = equity_burden[\"equity_burden\"].fillna(0)\n",
    "equity_burden.sort_values(by=\"equity_burden\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_equity.sort_values(by=\"total_loss\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "highlight = [\n",
    "    'Chosewood Park, Englewood Manor',\n",
    "    'Grove Park',\n",
    "    'Browns Mill Park, Polar Rock, Swallow Circle/Baywood',\n",
    "    'Bush Mountain, Oakland City',\n",
    "]\n",
    "utils.map(\n",
    "    equity_burden,\n",
    "    color=\"equity_burden\",\n",
    "    title=\"\",\n",
    "    nbhd_df=nsa_geo,\n",
    "    ax=ax,\n",
    "    highlight=highlight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_burden.sort_values(\"equity_burden\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_burden[\"income_generated\"].sum() - per_year_nsa[per_year_nsa[\"Year\"] != 2012][\"tAggregateHHInc_m\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_summary.sort_values(\"equity_burden\", ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_equity.agg({\"sale_loss\": sum, \"purchase_loss\": sum, \"rental_loss\": sum, \"total_loss\": sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_equity[\"mb_flag\"] = total_equity[\"neighborhood\"].apply(lambda x: 1 if x in mb_neighborhoods else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_equity.groupby(\"mb_flag\")[\"total_loss_adj\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average equity burden for mb and non-mb neighborhoods (sum mb generation, mb generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_burden[\"mb_flag\"] = equity_burden[\"neighborhood\"].apply(lambda x: 1 if x in mb_neighborhoods else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_burden.groupby(\"mb_flag\")[\"income_generated\"].sum() * (digest[\"TAXYR\"].nunique() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_burden.groupby(\"mb_flag\")[\"total_loss\"].sum() / equity_burden.groupby(\"mb_flag\")[\"income_generated\"].sum() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_summary.sort_values(\"equity_burden\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_burden[equity_burden[\"neighborhood\"].str.contains(\"Bush\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change median to avg household income... neeed to change for BUsh mountain in section too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum equity loss for each year by nbhd type\n",
    "# sum household size * total households for each year by nbhd type\n",
    "total_loss = equity_burden.groupby(\"mb_flag\")[\"total_loss\"].sum()\n",
    "total_generation = equity_burden.groupby(\"mb_flag\")[\"income_generated\"].sum() * (digest[\"TAXYR\"].nunique() - 1)\n",
    "\n",
    "overall_pct = pd.concat([total_loss, total_generation], axis=1)\n",
    "overall_pct[\"pct\"] = overall_pct[\"total_loss\"] / overall_pct[\"income_generated\"] * 100\n",
    "overall_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Statistical Tests: Corporate Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivating question:** is the price paid (and sale_diff) different by sale type?\n",
    "\n",
    "sale_diff = fmv_adj - price_adj\n",
    "\n",
    "sale_type = corp to ind, ind to corp, etc.\n",
    "\n",
    "**Model type**: Mixed Linear Model\n",
    "\n",
    "**Model characteristics**\n",
    "- Groups: neighborhoods and sale year (need to drop first year to avoid dummy trap)\n",
    "- Treatment: type of sale (also need to avoid dummy trap, reference param in statsmodels library should do this)\n",
    "- Dependent vars: sales price (log), sales_diff\n",
    "- Covariates: \n",
    "    - Fixed effect: whether sale is valid, num beds, num baths, sqft living area, year built, heat\n",
    "    - Random effect: sale year as level 1, neighborhoods nested within as level 2 - ADD TO MODEL\n",
    "\n",
    "**Notes**\n",
    "- Allow slopes and intercepts to vary?\n",
    "- add in taxyr dummy, omitting first year (0): need to drop first year\n",
    "- num bath, num bed, amenities, lot size, AC or not\n",
    "- cluster standard errors at nbhd level\n",
    "\n",
    "**Steps (do separately for each DV)**\n",
    "- **Run model**\n",
    "- **Residuals vs fitted plot (entire model):** verify there are no patterns, should be randomly distributed around zero\n",
    "- **Residuals for each predictor variable:** verify no patterns\n",
    "- **Distribution of residuals (QQ plot):** verify normal\n",
    "- **Distribution of random effects (QQ plot):** verify normal\n",
    "- **Variance inflation factors of IVs:** verify none are above 5-10\n",
    "- **Correlation matrix of IVs:** verify all below 0.8\n",
    "- **Degrees of Freedom:**\n",
    "- **Confidence Intervals:** verify CIs don't include 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data: cast all int/float, take log of sales price, create dummy column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use heat, 4 = Central Air, 3 is forced air, 2 non central, 1 is none\n",
    "# from understanding the PRC\n",
    "sales_stat_vars = {\n",
    "    \"Sale Year\": \"int\",\n",
    "    \"neighborhood\": \"string\",\n",
    "    \"corp_sold_ind\": \"int\",\n",
    "    \"corp_bought_ind\": \"int\",\n",
    "    \"ind_to_ind\": \"int\",\n",
    "    \"corp_to_corp\": \"int\",\n",
    "    \"Saleval\": \"string\",\n",
    "    \"sales_price\": \"float\",\n",
    "    \"FAIR MARKET VALUE\": \"float\",\n",
    "    \"price_adj\": \"float\",\n",
    "    \"sale_diff\": \"float\",\n",
    "    \"sqft_living\": \"float\",\n",
    "    \"yr_built\": \"int\",\n",
    "    \"beds\": \"int\",\n",
    "    \"baths\": \"int\",\n",
    "    \"acres\": \"float\",\n",
    "    \"heat\": \"int\",\n",
    "    \"D Effyr\": \"int\",\n",
    "    \"Extwall\": \"int\",\n",
    "    \"Style\": \"string\",\n",
    "    \"Rmtot\": \"int\",\n",
    "    \"D Grade\": \"string\",\n",
    "    \"Bsmt\": \"int\"\n",
    "}\n",
    "sale_type_names = [\n",
    "    \"ind_to_ind\",\n",
    "    \"corp_to_corp\",\n",
    "    \"corp_bought_ind\",\n",
    "    \"corp_sold_ind\",\n",
    "]\n",
    "sales_atl = sales[sales[\"neighborhood\"].notna()]\n",
    "sales_atl = sales_atl.merge(digest[\n",
    "    [\"TAXYR\", \"PARID\", \"yr_built\", \"sqft_living\", \"beds\", \"baths\", \"acres\", \"heat\", \"D Effyr\", \"Extwall\", \"Style\", \"Rmtot\", \"D Grade\", \"Bsmt\"]\n",
    "], on=[\"PARID\", \"TAXYR\"], how=\"left\")\n",
    "sales_for_stat = sales_atl[sales_stat_vars.keys()].copy()\n",
    "init_len = len(sales_for_stat)\n",
    "sales_for_stat = sales_for_stat.dropna()\n",
    "print(f\"Dropped {init_len - len(sales_for_stat)} records due to missing values from digest merge\")\n",
    "sales_for_stat[\"sale_type\"] = pd.from_dummies(sales_for_stat[sale_type_names])\n",
    "sales_for_stat[\"valid_sale\"] = sales_for_stat.apply(\n",
    "    lambda x: x[\"Saleval\"] == \"0\",\n",
    "    axis=1\n",
    ").astype(int)\n",
    "\n",
    "sales_for_stat[\"sale_type\"] = pd.from_dummies(sales_for_stat[sale_type_names])\n",
    "\n",
    "for col, dtype in sales_stat_vars.items():\n",
    "    sales_for_stat[col] = sales_for_stat[col].astype(dtype)\n",
    "\n",
    "# natural log of sales price (adj for inflation)\n",
    "sales_for_stat[\"price_adj_log\"] = np.log(sales_for_stat[\"price_adj\"])\n",
    "sales_for_stat.rename(columns={\"Sale Year\": \"sale_year\"}, inplace=True)\n",
    "\n",
    "# save data for validation\n",
    "#sales_for_stat.to_csv(\"output/sales_for_stat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Check for Dist of sale_diff: fmv_adj - price_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_for_stat[\"sale_diff\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df, formula, groups, re_formula=\"1\"):\n",
    "    formula = formula\n",
    "    model = smf.mixedlm(formula, df, groups=df[groups], re_formula=re_formula)\n",
    "    return model.fit()\n",
    "\n",
    "def verbose_output(result):\n",
    "    print(result.summary())\n",
    "    print(result.random_effects)\n",
    "    print(result.cov_re)\n",
    "    print(result.cov_params())\n",
    "    \n",
    "    # for name, coef in zip(mixed_lm_result.model.exog_names, mixed_lm_result.params):\n",
    "    #     percent_change = (np.exp(coef) - 1) * 100\n",
    "    #     print(f\"{name}: {percent_change:.2f}%\")\n",
    "    \n",
    "def model_residuals(result):\n",
    "    fitted_vals = result.fittedvalues\n",
    "    residuals = result.resid\n",
    "    plt.scatter(fitted_vals, residuals)\n",
    "    plt.xlabel('Fitted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.show()\n",
    "    \n",
    "def predictor_residuals(df, result, predictor):\n",
    "    residuals = result.resid\n",
    "    plt.scatter(df[predictor], residuals)\n",
    "    plt.xlabel(f'{predictor}')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.show()\n",
    "    \n",
    "def dist_residuals(result):\n",
    "    residuals = result.resid\n",
    "    fig = sm.qqplot(residuals, stats.t, fit=True, line=\"45\")\n",
    "    plt.show()\n",
    "    \n",
    "def dist_random_effects(result):\n",
    "    random_effects = result.random_effects\n",
    "    # Flatten the array for plotting\n",
    "    #re_flat = np.concatenate([re.flatten() for re in random_effects.values()])\n",
    "    print(random_effects)\n",
    "    fig = sm.qqplot(random_effects, stats.t, fit=True, line=\"45\")\n",
    "    plt.show()\n",
    "    \n",
    "def vif(df, predictors):\n",
    "    pred = df[predictors]\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(pred.values, i) for i in range(pred.shape[1])]\n",
    "    vif[\"features\"] = pred.columns\n",
    "    print(vif)\n",
    "    \n",
    "def corr_matrix(df, predictors):\n",
    "    pred = df[predictors]\n",
    "    corr_matrix = pred.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "def run_diagnostics(df, result, predictors):\n",
    "    model_residuals(result)\n",
    "    \n",
    "    for predictor in predictors:\n",
    "        predictor_residuals(df, result, predictor)\n",
    "        \n",
    "    dist_residuals(result)\n",
    "    #dist_random_effects(result)\n",
    "    #dist_random_effects(result, predictors)\n",
    "    vif(df, predictors)\n",
    "    corr_matrix(df, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Sales Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'price_adj_log ~ C(sale_type, Treatment(reference=\"ind_to_ind\")) + C(valid_sale) + beds + baths + sqft_living + yr_built + C(heat) + C(sale_year)'\n",
    "result = run_model(df=sales_for_stat, formula=formula, groups=\"neighborhood\")\n",
    "verbose_output(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_for_stat[\"price_adj_log\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, coef in zip(result.model.exog_names, result.params):\n",
    "    percent_change = (np.exp(coef) - 1) * 100\n",
    "    print(f\"{name}: {percent_change:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_diagnostics(sales_for_stat, result, [\"beds\", \"baths\", \"sqft_living\", \"yr_built\", \"heat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Sale Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'sale_diff ~ sale_type + valid_sale + num_beds + num_baths + sqft_living + year_built + heat'\n",
    "result = run_model(df=sales_for_stat, formula=formula, groups=\"neighborhood\", re_formula=\"~sale_year\")\n",
    "verbose_output(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_diagnostics(sales_for_stat, result, [\"num_beds\", \"num_baths\", \"sqft_living\", \"year_built\", \"heat\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equity-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
